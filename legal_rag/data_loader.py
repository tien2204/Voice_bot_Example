# data_loader.py - YuITC Dataset Loader
# =====================================

import re
from typing import List, Dict, Any, Optional
from datasets import load_dataset
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
import json
import hashlib

class YuITCDataLoader:
    """
    Loader cho YuITC Vietnamese Legal Document Retrieval Data
    Dataset structure: question, context_list, qid, cid
    """
    
    def __init__(self, chunk_size: int = 512, chunk_overlap: int = 100, max_documents: int = None):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.max_documents = max_documents
        
        # Text splitter cho chunking
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=[
                "\n\nƒêi·ªÅu ",  # ∆Øu ti√™n chia theo ƒëi·ªÅu lu·∫≠t
                "\n\nKho·∫£n ",  # Chia theo kho·∫£n
                "\n\nƒêi·ªÉm ",   # Chia theo ƒëi·ªÉm
                "\n\n",        # Chia theo paragraph
                "\n",          # Chia theo d√≤ng
                ". ",          # Chia theo c√¢u
                "! ",
                "? ",
                " ",           # Chia theo t·ª´
                ""
            ]
        )
        
        self.legal_pattern = self._compile_legal_patterns()
    
    def _compile_legal_patterns(self):
        """Compile regex patterns ƒë·ªÉ nh·∫≠n di·ªán c·∫•u tr√∫c ph√°p lu·∫≠t"""
        patterns = {
            'dieu': re.compile(r'ƒêi·ªÅu\s+(\d+)\.?\s*(.+?)(?=ƒêi·ªÅu\s+\d+|$)', re.DOTALL),
            'khoan': re.compile(r'(\d+)\.?\s*(.+?)(?=\d+\.|$)', re.DOTALL),
            'diem': re.compile(r'([a-z])\)\s*(.+?)(?=[a-z]\)|$)', re.DOTALL),
            'luat': re.compile(r'(Lu·∫≠t|Ngh·ªã ƒë·ªãnh|Th√¥ng t∆∞|Quy·∫øt ƒë·ªãnh)\s+([^,\n]+)', re.IGNORECASE),
            'so_van_ban': re.compile(r'(\d+/\d+/[A-Z-]+)', re.IGNORECASE)
        }
        return patterns
    
    def load_dataset(self) -> List[Document]:
        """
        Load YuITC dataset v√† convert th√†nh documents
        """
        print("ƒêang t·∫£i YuITC Vietnamese Legal Document Retrieval Data...")
        
        try:
            # Load dataset t·ª´ HuggingFace
            dataset = load_dataset(
                "YuITC/Vietnamese-Legal-Doc-Retrieval-Data",
                split="train"
            )
            
            print(f"ƒê√£ t·∫£i dataset v·ªõi {len(dataset)} records")
            
            # Process dataset
            documents = []
            context_cache = set()  # ƒê·ªÉ tr√°nh duplicate context
            
            for idx, item in enumerate(dataset):
                if self.max_documents and idx >= self.max_documents:
                    break
                
                # Extract data t·ª´ item
                question = item.get('question', '')
                context_list = item.get('context_list', [])
                qid = item.get('qid', f'q_{idx}')
                cid = item.get('cid', [])
                
                # Process context_list
                processed_docs = self._process_context_list(
                    context_list, qid, cid, question, idx
                )
                
                # Add unique contexts
                for doc in processed_docs:
                    content_hash = hashlib.md5(doc.page_content.encode()).hexdigest()
                    if content_hash not in context_cache:
                        context_cache.add(content_hash)
                        documents.append(doc)
                
                if (idx + 1) % 100 == 0:
                    print(f"üìñ ƒê√£ x·ª≠ l√Ω {idx + 1}/{min(len(dataset), self.max_documents or len(dataset))} records...")
            
            print(f"T·ªïng c·ªông t·∫°o ra {len(documents)} chunks t·ª´ dataset")
            return documents
            
        except Exception as e:
            print(f"L·ªói khi t·∫£i YuITC dataset: {e}")
            print("T·∫°o d·ªØ li·ªáu m·∫´u...")
            return self._create_sample_data()
    
    def _process_context_list(self, context_list: List[str], qid: str, cid: List[str], 
                            question: str = "", item_idx: int = 0) -> List[Document]:
        """
        Process context_list th√†nh chunks
        """
        documents = []
        
        for ctx_idx, context in enumerate(context_list):
            if not context or not isinstance(context, str):
                continue
                
            context = context.strip()
            if len(context) < 50:  # Skip context qu√° ng·∫Øn
                continue
            
            # Detect lo·∫°i vƒÉn b·∫£n ph√°p lu·∫≠t
            doc_type = self._detect_document_type(context)
            
            # Chunking strategy
            if doc_type in ['luat', 'nghi_dinh'] and len(context) > self.chunk_size:
                # Chia theo c·∫•u tr√∫c ph√°p lu·∫≠t
                chunks = self._chunk_by_legal_structure(context)
            else:
                # Chia theo token size th√¥ng th∆∞·ªùng
                chunks = self.text_splitter.split_text(context)
            
            # Create Document objects
            for chunk_idx, chunk in enumerate(chunks):
                if len(chunk.strip()) < 30:  # Skip chunk qu√° ng·∫Øn
                    continue
                
                # Extract metadata
                metadata = self._extract_metadata(chunk, context, doc_type)
                metadata.update({
                    'qid': qid,
                    'original_cid': cid[ctx_idx] if ctx_idx < len(cid) else f'c_{item_idx}_{ctx_idx}',
                    'context_idx': ctx_idx,
                    'chunk_idx': chunk_idx,
                    'source': f"yu√≠tc_item_{item_idx}_ctx_{ctx_idx}_chunk_{chunk_idx}",
                    'related_question': question,
                    'document_type': doc_type,
                    'chunk_size': len(chunk),
                    'original_context_size': len(context)
                })
                
                doc = Document(
                    page_content=chunk.strip(),
                    metadata=metadata
                )
                
                documents.append(doc)
        
        return documents
    
    def _detect_document_type(self, text: str) -> str:
        """
        Detect lo·∫°i vƒÉn b·∫£n ph√°p lu·∫≠t
        """
        text_upper = text.upper()
        
        if any(keyword in text_upper for keyword in ['LU·∫¨T', 'LAW']):
            return 'luat'
        elif any(keyword in text_upper for keyword in ['NGH·ªä ƒê·ªäNH', 'DECREE']):
            return 'nghi_dinh'
        elif any(keyword in text_upper for keyword in ['TH√îNG T∆Ø', 'CIRCULAR']):
            return 'thong_tu'
        elif any(keyword in text_upper for keyword in ['QUY·∫æT ƒê·ªäNH', 'DECISION']):
            return 'quyet_dinh'
        elif any(keyword in text_upper for keyword in ['B·ªò LU·∫¨T', 'CODE']):
            return 'bo_luat'
        elif 'HI·∫æN PH√ÅP' in text_upper:
            return 'hien_phap'
        else:
            return 'van_ban_khac'
    
    def _chunk_by_legal_structure(self, text: str) -> List[str]:
        """
        Chia chunk theo c·∫•u tr√∫c ph√°p lu·∫≠t (ƒêi·ªÅu, Kho·∫£n, ƒêi·ªÉm)
        """
        chunks = []
        
        # Th·ª≠ chia theo ƒêi·ªÅu tr∆∞·ªõc
        dieu_matches = list(self.legal_pattern['dieu'].finditer(text))
        
        if len(dieu_matches) > 1:
            # C√≥ nhi·ªÅu ƒëi·ªÅu -> chia theo ƒëi·ªÅu
            for i, match in enumerate(dieu_matches):
                dieu_num = match.group(1)
                dieu_content = match.group(2).strip()
                
                chunk_title = f"ƒêi·ªÅu {dieu_num}"
                chunk_content = f"{chunk_title}. {dieu_content}"
                
                # N·∫øu ƒëi·ªÅu qu√° d√†i, chia nh·ªè h∆°n n·ªØa
                if len(chunk_content) > self.chunk_size * 1.5:
                    sub_chunks = self.text_splitter.split_text(chunk_content)
                    chunks.extend(sub_chunks)
                else:
                    chunks.append(chunk_content)
        else:
            # Kh√¥ng c√≥ c·∫•u tr√∫c ƒëi·ªÅu r√µ r√†ng -> d√πng text_splitter th∆∞·ªùng
            chunks = self.text_splitter.split_text(text)
        
        return [chunk for chunk in chunks if chunk.strip()]
    
    def _extract_metadata(self, chunk: str, original_context: str, doc_type: str) -> Dict[str, Any]:
        """
        Extract metadata t·ª´ chunk
        """
        metadata = {}
        
        # Extract s·ªë ƒëi·ªÅu n·∫øu c√≥
        dieu_match = self.legal_pattern['dieu'].search(chunk)
        if dieu_match:
            metadata['dieu_so'] = dieu_match.group(1)
        
        # Extract t√™n lu·∫≠t/vƒÉn b·∫£n
        luat_match = self.legal_pattern['luat'].search(chunk)
        if luat_match:
            metadata['loai_van_ban'] = luat_match.group(1)
            metadata['ten_van_ban'] = luat_match.group(2)
        
        # Extract s·ªë vƒÉn b·∫£n
        so_match = self.legal_pattern['so_van_ban'].search(chunk)
        if so_match:
            metadata['so_van_ban'] = so_match.group(1)
        
        # T√≠nh to√°n th·ªëng k√™
        metadata['word_count'] = len(chunk.split())
        metadata['char_count'] = len(chunk)
        
        # Check n·∫øu l√† chunk ƒë·∫ßu c·ªßa document
        metadata['is_first_chunk'] = chunk in original_context[:len(chunk)]
        
        return metadata
    
    def _create_sample_data(self) -> List[Document]:
        """
        T·∫°o d·ªØ li·ªáu m·∫´u phong ph√∫ v·ªÅ ph√°p lu·∫≠t Vi·ªát Nam
        """
        sample_contexts = [
            {
                "context": "ƒêi·ªÅu 1. Ph·∫°m vi ƒëi·ªÅu ch·ªânh\n1. B·ªô lu·∫≠t n√†y quy ƒë·ªãnh v·ªÅ t·ªôi ph·∫°m v√† h√¨nh ph·∫°t.\n2. B·ªô lu·∫≠t n√†y √°p d·ª•ng ƒë·ªëi v·ªõi m·ªçi ng∆∞·ªùi ph·∫°m t·ªôi tr√™n l√£nh th·ªï n∆∞·ªõc C·ªông h√≤a x√£ h·ªôi ch·ªß nghƒ©a Vi·ªát Nam.\nƒêi·ªÅu 2. Nhi·ªám v·ª• c·ªßa B·ªô lu·∫≠t h√¨nh s·ª±\nB·ªô lu·∫≠t h√¨nh s·ª± c√≥ nhi·ªám v·ª• b·∫£o v·ªá ƒë·ªôc l·∫≠p, ch·ªß quy·ªÅn, th·ªëng nh·∫•t, to√†n v·∫πn l√£nh th·ªï c·ªßa T·ªï qu·ªëc, b·∫£o v·ªá ch·∫ø ƒë·ªô ch√≠nh tr·ªã.",
                "question": "B·ªô lu·∫≠t h√¨nh s·ª± c√≥ ph·∫°m vi ƒëi·ªÅu ch·ªânh nh∆∞ th·∫ø n√†o?",
                "doc_type": "bo_luat"
            },
            {
                "context": "ƒêi·ªÅu 15. Tu·ªïi ch·ªãu tr√°ch nhi·ªám h√¨nh s·ª±\n1. Ng∆∞·ªùi t·ª´ ƒë·ªß 16 tu·ªïi tr·ªü l√™n ph·∫£i ch·ªãu tr√°ch nhi·ªám h√¨nh s·ª± v·ªÅ m·ªçi t·ªôi ph·∫°m.\n2. Ng∆∞·ªùi t·ª´ ƒë·ªß 14 tu·ªïi ƒë·∫øn d∆∞·ªõi 16 tu·ªïi ph·∫£i ch·ªãu tr√°ch nhi·ªám h√¨nh s·ª± v·ªÅ t·ªôi r·∫•t nghi√™m tr·ªçng do c·ªë √Ω, t·ªôi ƒë·∫∑c bi·ªát nghi√™m tr·ªçng quy ƒë·ªãnh t·∫°i c√°c ƒëi·ªÅu 123, 134, 141, 142, 143, 144, 150, 151, 168, 169, 170, 171, 173, 174, 178, 248, 249, 250, 251, 252, 266, 286, 287, 289, 290, 299, 303, 304 c·ªßa B·ªô lu·∫≠t n√†y.",
                "question": "Tu·ªïi ch·ªãu tr√°ch nhi·ªám h√¨nh s·ª± ƒë∆∞·ª£c quy ƒë·ªãnh nh∆∞ th·∫ø n√†o?",
                "doc_type": "bo_luat"
            },
            {
                "context": "ƒêi·ªÅu 33. T√π chung th√¢n\n1. T√π chung th√¢n l√† h√¨nh ph·∫°t t√π kh√¥ng th·ªùi h·∫°n.\n2. T√π chung th√¢n ch·ªâ √°p d·ª•ng ƒë·ªëi v·ªõi ng∆∞·ªùi ph·∫°m t·ªôi ƒë·∫∑c bi·ªát nghi√™m tr·ªçng trong tr∆∞·ªùng h·ª£p kh√¥ng √°p d·ª•ng h√¨nh ph·∫°t t·ª≠ h√¨nh v√† ƒë∆∞·ª£c quy ƒë·ªãnh t·∫°i c√°c ƒëi·ªÅu c·ªßa Ph·∫ßn ƒë·∫∑c bi·ªát c·ªßa B·ªô lu·∫≠t n√†y.\n3. T√π chung th√¢n kh√¥ng √°p d·ª•ng ƒë·ªëi v·ªõi ng∆∞·ªùi d∆∞·ªõi 18 tu·ªïi khi ph·∫°m t·ªôi; ph·ª• n·ªØ c√≥ thai; ng∆∞·ªùi t·ª´ ƒë·ªß 70 tu·ªïi tr·ªü l√™n.",
                "question": "H√¨nh ph·∫°t t√π chung th√¢n ƒë∆∞·ª£c √°p d·ª•ng nh∆∞ th·∫ø n√†o?",
                "doc_type": "bo_luat"
            },
            {
                "context": "ƒêi·ªÅu 143. T·ªôi gi·∫øt ng∆∞·ªùi\n1. Ng∆∞·ªùi n√†o c·ªë √Ω l√†m ch·∫øt ng∆∞·ªùi kh√°c, th√¨ b·ªã ph·∫°t t√π t·ª´ 12 nƒÉm ƒë·∫øn 20 nƒÉm, t√π chung th√¢n ho·∫∑c t·ª≠ h√¨nh.\n2. Ph·∫°m t·ªôi thu·ªôc m·ªôt trong c√°c tr∆∞·ªùng h·ª£p sau ƒë√¢y, th√¨ b·ªã ph·∫°t t√π t·ª´ 20 nƒÉm, t√π chung th√¢n ho·∫∑c t·ª≠ h√¨nh:\na) C√≥ t·ªï ch·ª©c;\nb) Gi·∫øt nhi·ªÅu ng∆∞·ªùi;\nc) Gi·∫øt ng∆∞·ªùi d∆∞·ªõi 16 tu·ªïi;\nd) Gi·∫øt ph·ª• n·ªØ m√† bi·∫øt l√† c√≥ thai;\nƒë) Gi·∫øt ng∆∞·ªùi trong t√¨nh tr·∫°ng th·∫ßn kinh b·∫•t th∆∞·ªùng do t√°c ƒë·ªông c·ªßa ch·∫•t ma t√∫y.",
                "question": "T·ªôi gi·∫øt ng∆∞·ªùi c√≥ nh·ªØng m·ª©c ph·∫°t nh∆∞ th·∫ø n√†o?",
                "doc_type": "bo_luat"
            },
            {
                "context": "ƒêi·ªÅu 1. Ph·∫°m vi ƒëi·ªÅu ch·ªânh\nLu·∫≠t n√†y quy ƒë·ªãnh v·ªÅ h√¥n nh√¢n v√† gia ƒë√¨nh; quy·ªÅn v√† nghƒ©a v·ª• c·ªßa th√†nh vi√™n gia ƒë√¨nh; h√¥n nh√¢n v√† gia ƒë√¨nh c√≥ y·∫øu t·ªë n∆∞·ªõc ngo√†i.\nƒêi·ªÅu 2. Nguy√™n t·∫Øc c·ªßa h√¥n nh√¢n v√† gia ƒë√¨nh\n1. H√¥n nh√¢n t·ª± do, ti·∫øn b·ªô, m·ªôt v·ª£ m·ªôt ch·ªìng, v·ª£ ch·ªìng b√¨nh ƒë·∫≥ng.\n2. Gia ƒë√¨nh b√¨nh ƒë·∫≥ng, h√≤a thu·∫≠n, h·∫°nh ph√∫c v√† b·ªÅn v·ªØng.",
                "question": "Lu·∫≠t h√¥n nh√¢n v√† gia ƒë√¨nh c√≥ nh·ªØng nguy√™n t·∫Øc g√¨?",
                "doc_type": "luat"
            },
            {
                "context": "ƒêi·ªÅu 8. ƒêi·ªÅu ki·ªán k·∫øt h√¥n\n1. Nam t·ª´ ƒë·ªß 20 tu·ªïi, n·ªØ t·ª´ ƒë·ªß 18 tu·ªïi.\n2. Vi·ªác k·∫øt h√¥n do nam, n·ªØ t·ª± nguy·ªán quy·∫øt ƒë·ªãnh.\n3. Kh√¥ng b·ªã c·∫•m k·∫øt h√¥n theo quy ƒë·ªãnh t·∫°i ƒêi·ªÅu 10 c·ªßa Lu·∫≠t n√†y.\nƒêi·ªÅu 9. ƒêƒÉng k√Ω k·∫øt h√¥n\n1. Vi·ªác k·∫øt h√¥n ph·∫£i ƒë∆∞·ª£c ƒëƒÉng k√Ω t·∫°i c∆° quan c√≥ th·∫©m quy·ªÅn.\n2. Ch·ªâ c√¥ng nh·∫≠n h√¥n nh√¢n ƒë∆∞·ª£c ƒëƒÉng k√Ω theo quy ƒë·ªãnh c·ªßa ph√°p lu·∫≠t.",
                "question": "ƒêi·ªÅu ki·ªán v√† th·ªß t·ª•c k·∫øt h√¥n ƒë∆∞·ª£c quy ƒë·ªãnh nh∆∞ th·∫ø n√†o?",
                "doc_type": "luat"
            }
        ]
        
        documents = []
        
        for idx, item in enumerate(sample_contexts):
            context = item["context"]
            question = item["question"]
            doc_type = item["doc_type"]
            
            # Chunk context if needed
            if len(context) > self.chunk_size:
                chunks = self._chunk_by_legal_structure(context)
            else:
                chunks = [context]
            
            for chunk_idx, chunk in enumerate(chunks):
                metadata = self._extract_metadata(chunk, context, doc_type)
                metadata.update({
                    'qid': f'sample_q_{idx}',
                    'original_cid': f'sample_c_{idx}',
                    'context_idx': 0,
                    'chunk_idx': chunk_idx,
                    'source': f"sample_legal_{idx}_chunk_{chunk_idx}",
                    'related_question': question,
                    'document_type': doc_type,
                    'chunk_size': len(chunk),
                    'original_context_size': len(context),
                    'is_sample_data': True
                })
                
                doc = Document(
                    page_content=chunk.strip(),
                    metadata=metadata
                )
                
                documents.append(doc)
        
        print(f"‚úÖ ƒê√£ t·∫°o {len(documents)} sample documents v·ªÅ ph√°p lu·∫≠t Vi·ªát Nam")
        return documents
    
    def get_statistics(self, documents: List[Document]) -> Dict[str, Any]:
        """
        T√≠nh to√°n th·ªëng k√™ v·ªÅ dataset
        """
        if not documents:
            return {}
        
        doc_types = {}
        chunk_sizes = []
        word_counts = []
        has_dieu = 0
        
        for doc in documents:
            # Document type stats
            doc_type = doc.metadata.get('document_type', 'unknown')
            doc_types[doc_type] = doc_types.get(doc_type, 0) + 1
            
            # Size stats
            chunk_sizes.append(doc.metadata.get('chunk_size', len(doc.page_content)))
            word_counts.append(doc.metadata.get('word_count', len(doc.page_content.split())))
            
            # Structure stats
            if doc.metadata.get('dieu_so'):
                has_dieu += 1
        
        return {
            'total_documents': len(documents),
            'document_types': doc_types,
            'avg_chunk_size': sum(chunk_sizes) / len(chunk_sizes) if chunk_sizes else 0,
            'avg_word_count': sum(word_counts) / len(word_counts) if word_counts else 0,
            'max_chunk_size': max(chunk_sizes) if chunk_sizes else 0,
            'min_chunk_size': min(chunk_sizes) if chunk_sizes else 0,
            'documents_with_dieu': has_dieu,
            'coverage_percentage': (has_dieu / len(documents)) * 100 if documents else 0
        }

if __name__ == "__main__":
    # Test data loader
    loader = YuITCDataLoader(chunk_size=512, chunk_overlap=100, max_documents=10)
    documents = loader.load_dataset()
    
    print(f"\n Dataset Statistics:")
    stats = loader.get_statistics(documents)
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    print(f"\n Sample Documents:")
    for i, doc in enumerate(documents[:3]):
        print(f"\n--- Document {i+1} ---")
        print(f"Source: {doc.metadata.get('source', 'Unknown')}")
        print(f"Type: {doc.metadata.get('document_type', 'Unknown')}")
        print(f"Content: {doc.page_content[:200]}...")
        print(f"Metadata: {dict(list(doc.metadata.items())[:5])}")  # First 5 metadata items